**建模与结构**

- ==不用大脑做类比的快速简介==
- ==单个神经元建模==
    
    1. ==生物动机和连接==
    2. ==作为线性分类器的单个神经元==
    3. ==常用的激活函数==
- ==神经网络结构==
    
    1. ==层组织==
    2. ==前向传播计算例子==
    3. ==表达能力==
    4. ==设置层的数量和尺寸==
- ==小节==
- ==参考文献== 
--简介：线性分类 s=Wx  
--在神经元输出端加损失函数  
二分类softmax分类器，预测标准是输出是否大于0.5  
二分类SVM分类器 加折叶损失函数  
正则化：生物上时逐渐遗忘，让权重w趋于0  
--**一个单独的神经元可以用来实现一个二分类分类器**

- sigmod【0-1】 1.函数饱和使梯度消失，在接近0或1时会饱和，要留意权重初始化，不能太大。 2.输出不是零中心的，导致权重更新时呈Z字型。
- tanh【-1-1】也存在饱和问题，但输出中心是零
- ReLU _f=max(0,x)_ 一个关于0的阈值

优点：收敛速度快，没有指数运算(**线性操作，不饱和**)  
缺点：大梯度流过时可能会提高其阈值，导致好多神经元无法被激活。

- Leaky ReLU
    
    - ![Exported image](Exported%20image%2020240403195631-0.png)
- Maxout
    
    - ![Exported image](Exported%20image%2020240403195631-1.png)

是上面两种函数的一般化，但是参数数量是一倍

--在同一个网络中混合使用不同类型的神经元是非常少见的  
--==一句话==：“_那么该用那种呢？_”**用****ReLU****非线性函数**。注意设置好**学习率**，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，**不要再用****sigmoid****了**。也可以试试tanh，但是其效果应该不如ReLU或者Maxout
   

--神经网络可以叫做 ANN(artificial neural networks)或者 MLP(multi-layer perceptrons)，用 unit指代神经元  
--**输出层**一般是没有激活函数的  
--分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算； 全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数  
--神经网络是个函数近似器，更大的网络更好，需要和更强的正则化配合，以免**过拟合**。
 
神经元分类（非线性分类） s=W2max(0,W1)x  
W1和W2通过随机梯度下降学习到，其梯度通过 反向传播，链式法则求导计算出  
--激活函数：

![Exported image](Exported%20image%2020240403195631-2.png)   
**预处理，正则化和损失函数**

- ==设置数据和模型==
    
    1. ==数据预处理==
    2. ==权重初始化==
    3. ==批量归一化（Batch Normalization）==
    4. ==正则化（L2/L1/Maxnorm/Dropout）==
- ==损失函数==
- ==小结==

--均值减法(mean substraction)：对每个独立特征减去其平均值，将图像中心挪到原点。[**零中心化**]  
--归一化(normalization) 将数据限制在固定范围内，如(-1-1)  
先进行零中心化，然后每个维度除以其标准差。  
--PCA和白化：先零中心化，然会计算协方差矩阵，展示数据中的相关性结构。 principal component analysis 是留下方差最大的100维度，降低其相关性。  
--任何预处理策略只能在数据集数据上进行计算，**应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。**
 
==--====初始化不能为零但要接近零，以打破对称性。====(====假设输入和权重的平均值都是====0====)==  
==--==使用标准差为_sqrt(2.0/n)_的高斯分布来初始化权重，其中  
是输入的神经元数。例如用numpy可以写作：**w = np.random.randn(n) * sqrt(2.0/n)****。**  
--偏置的初始化数值通常为0  
--批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。
 
--正则化-防止过拟合-也可以减小搜索空间

- L2：向目标函数中增加

对大数值进行严厉惩罚，分散权重，但是权重以==w += -lambda * W==向着0线性下降。  
选择更多特征，这些特征接近于0

- L1：向目标函数增加

它会让权重向量在最优化的过程中变得稀疏(即非常接近0)  
产生少量特征，其他特征为0

- 最大范式约束：给权重设定上限，并使用投影梯度。这样学 习率过高时也没事儿。
- 随机失活(dropout)：随机失活的实现方法是让神经元以**超参数****p**的概率被激活或者被设置为0。

--损失函数：SVM(每个样本都有唯一正确标签)-折叶损失  
Softmax(每个样本都有唯一正确标签)-交叉熵 损失  
--标签集非常庞大时，标签用决策树进行表示  
--对每个标签进行独立的二值化，可以使得一个样本可以对应多个标签，例如一幅图可以从不同角度对应多个相互独立的标签。  
--回归问题：预测实数的值的问题，计算预测值与真实值之间的损失。L2平方范式，L1范式
   

**正则化的本质就是添加约束，并与目标函数放在一起，引入了正则化因子，因子越大则目标函数对结果影响越小****(****优化算法课程最后一章****)****。**

![Exported image](Exported%20image%2020240403195631-3.png) ![Exported image](Exported%20image%2020240403195631-4.png)

**动态部分****(****学习参数和搜索最优超参数****)**

- ==梯度检查==
- ==合理性（Sanity）检查==
- ==检查学习过程==
    
    - ==损失函数==
    - ==训练集与验证集准确率==
    - ==权重：更新比例==
    - ==每层的激活数据与梯度分布==
    - ==可视化== _译者注：上篇翻译截止处_
- ==参数更新==
    
    - ==一阶（随机梯度下降）方法，动量方法，Nesterov动量方法==
    - ==学习率退火==
    - ==二阶方法==
    - ==逐参数适应学习率方法（Adagrad，RMSProp）==
- ==超参数调优==
- ==评价==
    
    - ==模型集成==
- ==总结==
- ==拓展引用==

--梯度检查：将解析梯度与数值计算梯度进行比较。  
--合理性检查：寻找特定情况的正确损失值  
提高正则化强度时导致损失值变大  
对小数据子集过拟合  
--跟踪重要数值，以周期(epoches)为单位进行跟踪  
--损失值，其震荡程度与批尺寸有关，越小震荡越大

- 验证集的准确率低很多，有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。
- 验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。

--权重更新值的数量与全部值数量之间的比值
 
1. 普通更新：x+= - lenrning_rate *dx
2. 动量更新：v=mu*v - learning_rate*dx; x+= v
3. Nesterov：v=mu*v - learning_rate*d(x+mu*v); x+=v

--学习率退火：目的-使参数规律跳动，稳定到损失函数深窄的地方

- 随步数衰减-每进行几个周期就根据一些因素降低学习率
- 指数衰减
- 1/t 衰减

实践中**随步衰减的随机失活**更受欢迎  
为了减小运算内存，使用L-BFGS近似Hessian，矩阵并没有被运算。  
--逐参数适应学习率方法  
Adagrad 跟踪每个参数的梯度平方和，和归一化参数更新步长，但是过于激进且过早停止学习。

![cache learning_rate dx / (np.sqrt(cache) + eps) ](Exported%20image%2020240403195631-5.png)

RMSprop

![Exported image](Exported%20image%2020240403195631-6.png)

Adam

![betal*m + (1 betal)*dx beta2*v + (1 beta2)*(dx**2) learning_rate m / (np.sqrt(v) + eps) ](Exported%20image%2020240403195631-7.png)

- ==利用小批量数据对实现进行梯度检查，还要注意各种错误。==
- ==进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。==
- ==在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。==
- ==推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。==
- ==随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。==
- ==使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。==
- ==进行模型集成来获得额外的性能提高。== > 来自 <[https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit](https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit)>  

tips：使用中心化公式，误差变小(Taylor Formula)

![df(x) ](Exported%20image%2020240403195631-8.png)  

使用相对误差来比较

![- fill ](Exported%20image%2020240403195631-9.png)

使用双精度  
保持在浮点数的有效范围  
目标函数的不可导点(Kinks)  
使用少量数据点  
谨慎设置步长h  
在操作的特性模式中检查，先让网络学习一小段时间(预热)。  
不要让正则化吞没数据-先关掉正则化，再单独检查。  
关闭随机失活，数据扩张等**不确定操作**  
检查少量维度  
--验证集和训练集的准确率
 
--一阶方法(随机梯度下降)及更新方法

![Exported image](Exported%20image%2020240403195631-10.png) ![Exported image](Exported%20image%2020240403195631-11.png)

--二阶方法**（无学习率）**

![Exported image](Exported%20image%2020240403195631-12.png)

--结论
 > 来自 <[https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit](https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit)>  
   

SGD：  
目的：解决GD运算慢的缺点