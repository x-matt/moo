1. ==线性分类器简介==
2. ==线性评分函数==
3. ==阐明线性分类器== _译者注：上篇翻译截止处_
4. ==损失函数==
5. ==多类SVM==
6. ==Softmax分类器==
7. ==SVM和Softmax的比较==
8. ==基于Web的可交互线性分类器原型==
9. ==小结==
    
    - 评分函数(Score Function) 原始图像数据到类别分值的映射--基于参数
    - 损失函数(Lost Function) 用来量化预测分类标签的得分与真实标签之间一致性的
    - **最优化问题：****通过更新评分函数的参数来最小化损失函数值**
    - 线性映射 _f(xi,W,b)=Wxi+b_ _--__score function_ 可将W和b合并 _f(xi,W)=Wxi_
    
    W(K*D)[**权重**],xi(D*1),b(K*1)[**偏差向量**](影响输出，但与原始数据不相关)
    
    - 可以从D维空间理解，也可从二维空间理解，即为线性函数
    - **W****的每一行都相当于一个分类器**，将其看作K个模板，则与KNN不同的是，这个模板是学习出来的，KNN的模板则是取训练集中的某一个。使用内积计算向量之间的距离，而不是L1和L2。
    
    - 损失函数(Lost function)也叫 代价函数(cost function)或目标函数(objective function)，输出结果与真实值差距越大，则函数值越大。
    - 根本思维源头：多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。 [https://www.youtube.com/watch?v=3liCbRZPrZA](https://www.youtube.com/watch?v=3liCbRZPrZA) #critical
    - 支持向量机(SVM)是什么意思？ - 简之的回答 - 知乎
    
    [https://www.zhihu.com/question/21094489/answer/86273196](https://www.zhihu.com/question/21094489/answer/86273196)
    
    - ![scores for other classes delta score score for correct class ](Exported%20image%2020240403195625-0.jpeg)
    
    - MSVM-保证正确分类上的得分始终比不正确分类上的得分高出一个边界值
    - 损失函数公式 将其他模板得分与正确的作差，与边界值作比较，再与正确值比较得出损失值
    - 关于0的阈值函数：max(0,-) 折叶损失(hinge loss)
    - 正则化(Regularization)：对特定的权重W添加偏好，其他权重不增加，以消除模糊性 正则化函数R(W)，仅仅是基于权重的函数（平方元素求和）。【L2惩罚】
    - **Li=****数据损失****(data loss)+****正则化损失****(regularization loss)**
        
        - ![L— EL + AR(W) regularization IQ S S data loss ](Exported%20image%2020240403195625-1.png)
    - **正则化的目的：让权重更为分散，不让某一个影响结果，使W具有泛化能力，充分利用所有维度，避免过拟合。对偏差b不需要正则化。将取值限定在一定范围内，减少优化计算时的计算难度。**
    - 二元支持向量机(BSVM)，K=2的情况下的分类情况
    
    - softmax分类器，交叉熵损失(cross-entropy loss)-对评分进行指数化，再进行归一化，正则化越强，则分布函数越平均。实质上是对数值的压缩。
    - softmax注重在整体中所占的比重，与s中的每个值都息息相关，损失值总能更小，而SVM中设置了阈值，只要满足该值即可，不追求细化，是个局部目标化的方法。
    
    - **损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的**                  
入 为超参数

**SVM****：**  
将实例看成空间中的点  
找到超平面之后进行映射  
属于二类分类模型  
原则：最大间隔化  
确定最佳超平面时：只有支持向量起作用
 
- 硬间隔：等于条件
- 软间隔：引入松弛变量；惩罚系数
- 非线性SVM：核技巧
 
**数学模型：**

1. 输入不同直线（boundary）
2. 计算各个点到直线距离
3. 取距离中最小值，记为margin1、margin2、….
4. 取 arg max(margin_i)，则得出的自变量，w、b 就是最合适的边界线
    
    1. ![1 arg max ](Exported%20image%2020240403195625-2.png)

**推导过程：**[https://blog.csdn.net/u014433413/article/details/78427574](https://blog.csdn.net/u014433413/article/details/78427574)