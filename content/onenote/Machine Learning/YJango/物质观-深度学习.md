1. 基本变换：层
    - 数学式子：$\vec{y}=a(W*\vec{x}+b)$
    - 数学理解：输入→变换→输出
        1. 升维/降维
        2. 放大/缩小
        3. 旋转
        4. 平移 -- b来实现
        5. “弯曲”-- a( )来实现
		    每层神经网络的数学理解：用线性变换跟随着非线性变化，将输入空间投向另一个空间 #important
    - 物理理解：Wx是指通过组合形成新的物质，a()符号我们的世界都是非线性的
2. 理解视角
    - 线性可分视角：神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。
    - 增加节点数：增加维度，即增加线性转换能力。
    - 增加层数：增加激活函数的次数，即增加非线性转换次数。
    - 物质组成视角：神经网络的学习过程就是学习物质组成方式的过程。
    - 增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。
    - 增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。
3. 神经网络的训练
    - 如何训练：
        - Input(特征)--评分函数(带权重的线性回归)：$f=W*x+b$ - 用损失函数评价线性回归的好坏 - 利用最优化求出损失函数的最小值
        - 最优化方法：梯度下降法；让loss值沿梯度反方向移动来降低loss；一次移动多少是学习速率；
    - 梯度下降的问题
        - 局部极小值：我们要找全局最小值，但往往卡在局部最小值
        - 调节步伐：调节学习速率，是每次更新的步伐不同
            - 随机梯度下降（Stochastic Gradient Descent (SGD)：每次只更新一个样本所计算的梯度
            - 小批量梯度下降（Mini-batch gradient descent）：每次更新若干样本所计算的梯度的平均值
            - 动量（Momentum）：不仅仅考虑当前样本所计算的梯度；Nesterov动量（Nesterov Momentum）：Momentum的改进
            - Adagrad、RMSProp、Adadelta、Adam：这些方法都是训练过程中依照规则降低学习速率，部分也综合动量
        - 优化起点：合理初始化权重、预训练网络
            - 高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）
            - Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）
        - 梯度的计算：反向传播法
4. 表现原因

数学视角：线性可分  
物质视角：物质组成