1. focal loss 有没有了解？为什么说focal los比普通的loss效果好
    
    1. 问题：负样本数量太大，占总loss的大部分，而且多是容易分类的（类别不均衡）
    2. 解决：减少易分类样本的权重，使训练集中在难分类样本上面--RetinaNet
2. 项目介绍
3. batch normalize 是如何使用的，和dropout一般是如何搭配使用的 group normalization 不依赖batch_size
    
    1. dropout是将某一个输出置为0
    2. 用在全连接层比较多，隐藏层用得少
4. Linux定时命令
5. PCA算法的原理（pricipal components analysis）
    
    1. 目的：数据降维
    2. 原理：协方差--二维的中心化、
6. 贝叶斯的先决条件是什么样的
7. 随机森林是怎么用的
8. 介绍RPN的模式？FPN的金字塔模式的原理？
9. 介绍以下MASK RCNN
10. FASTER R-CNN和YOLOV3的区别即两个网络的特点
11. YOLOV3和V1之间的区别、为什么说YOLO把背景的东西识别成前景物品上的概率要小一点
12. dropout的作用
13. L1和L2正则化的原理，什么时候选择L1，什么时候选择L2

（这个问题答得不好，主管提示说，从几何形状上理解，L1正则化是一个菱形，L2是一个圆形。L1比较符合伯努利分布（0、1分布），L2更符合正态分布）

15. 为什么说vgg16里用2个3*3卷积，代替一个5*5网络？（答：两层网络会汇聚更多特征，且参数量少。但是计算代价会大一点）
16. 为什么说参数量减少了，但是运算的量却增多了？
17. 在调参得时候，遇到loss值一直不变的时候怎么办？
18. 如果出现梯度爆炸怎么能看出来？
    
    1. 模型无法从训练数据中获得更新
    2. 模型不稳定，损失变化显著
    3. 模型损失变成NaN
    4. 模型梯度快速变大
    5. 权重变成NaN
    6. 每个节点层的误差梯度值一直超过1.0
19. 出现梯度爆炸怎么解决？tensorflow里面对梯度进行裁剪的接口叫什么
    
    1. 重新设计网络
    2. 使用ReLU激活函数
    3. 使用长短期记忆网络
    4. 使用梯度截断
20. 说说你最近看的论文
21. 是否有训练过自己的一套卷积神经网络架构？是怎么思考的？目的是什么？
22. 如果让你做一个电商中的 Logo 识别，你会怎么做？

[https://www.cnblogs.com/Mrzhang3389/p/10164350.html](https://www.cnblogs.com/Mrzhang3389/p/10164350.html)