Residual learning（32-152层）--传话机制（第一个人直接传话给第四个人）  
思路：保留以前的信息  
残差学习解决了深度网络的退化问题（深度越深，误差反而越高）  
解决了梯度弥散和梯度爆炸问题 normalization initialization  
深度越深，梯度之间的相关性会越差，resnet保持的梯度之间的相关性
 
全连接层：与前一个每个神经元全部连接
 
Batch normalization（批量归一化）  
目的：将待处理的一部分数据放到梯度较高的地方，==加快训练速度==

![减 均 值 除 方 差 归 一 化 Batch N za 心 01 ](Exported%20image%2020240403195707-0.png)  
![original data zero-centered data normalized data ](Exported%20image%2020240403195707-1.png) ![输 人 ： 批 处 理 (mini-batch) 输 人 “ B ： { 0 。 输 出 ： 规 范 化 后 的 网 络 响 应 ： BN7.ø@i) 1 ： “ 乥 ： 雪 榨 / / 计 算 批 处 理 数 据 均 值 2 ： “ 鬲 乥 监 一 ） 2 / / 计 算 批 处 理 数 据 方 差 3 ： “ ． 凸 寻 旦 ． / / 规 范 化 0B + 4 ： 佑 “ + 0 一 BN7.ß@i) / / 尺 度 变 换 和 偏 移 5 ： return 学 习 的 参 数 7 和 、 3 ． 如 上 a 所 小 ， BN 步 骤 主 要 分 为 4 步 1 ． 求 莓 一 个 训 比 次 数 据 的 均 值 2 求 莓 一 个 训 练 批 次 数 据 的 方 差 3 ． 亻 吏 用 求 得 的 均 值 和 方 差 对 该 扌 比 次 的 训 数 啹 f 故 归 一 化 ， 获 得 0 ． 1 分 布 。 其 中 E 是 为 了 齑 免 除 数 为 0 时 所 使 用 的 微 ／ 」 ' 4 ． 尺 度 变 换 和 偏 侈 将 乘 以 1 调 整 数 值 大 ／ 一 卜 ， 冉 加 上 多 加 偏 移 后 得 到 汶 里 的 7 是 尺 度 因 子 ， 是 平 移 因 子 。 是 BN 的 精 髓 由 于 一 化 后 的 基 本 会 被 眼 制 在 正 态 分 布 下 ， 亻 吏 得 网 络 的 表 达 能 力 下 降 。 为 解 决 该 问 题 ， 我 们 引 入 两 个 新 的 参 数 7 ， 多 。 7 和 多 是 在 讠 川 练 时 网 络 自 己 学 习 得 到 的 。 ](Exported%20image%2020240403195707-2.png)  

![[ResNet.pdf]]