梯度弥散：BN解决  
梯度消失：ResNet解决
 
Relu 低维度特征坍塌：本质是一个特征过滤器，低维度特征分布到Relu激活带上的几率低  
特征退化：Relu堆0值得梯度为0，权重无法更新  
使用激活函数得原则

1. 对含有冗余信息得数据使用非线性激活，对不含冗余信息得数据使用线性激活
2. 两种激活交替使用，从而兼顾信息得非线性和完整性
3. 对内存消耗大的结构最好用在非冗余信息上
4. Resnet本质：降低数据中信息的冗余度；提取了identity-信息冗余度低；对其他信息进行Relu
5. 解决由非线性激活导致的反向传播梯度消失的窍门，就是要提高进行非线性激活的信息的冗余度 7. **梯度消失的问题本质上是激活函数的偏导数在链式求导中累乘导致越来越小**
8. resnet在反向传播时传递了一个1的梯度
 
Resnet 解决的问题：网络深的时候，反传回来梯度之间的相关性会变差
    
问题：当前的backbone模型的内存太大，限制了CNN在低功耗领域的应用
 
Group convolution  
实质：将convolution分成g个独立的组--通道数/g
 
**MobileNet** **V1**  
depthwise+pointwise  
α系数用以压缩网络  
放弃了池化层
 
**ShuffleNet** **V1**  
思路：用group convolution 和 channel shuffle 改进resnet  
不足：需要大量指针跳转和存储集，速度拖慢；规则是人工设计的，不是自己学的
 
**MobileNet V2**  
目的：解决V1在训练过程中容易特征退化的问题  
Relu造成低维度的特征坍塌  
没有复用特征  
添加了类似resnet的机制
 
CNN中经典案例  
[https://zhuanlan.zhihu.com/p/28749411](https://zhuanlan.zhihu.com/p/28749411)
 
1. Group convolution
    
    1. 为了解决硬件限制，将网络分多个GPU运算
    2. 同时大大减少了参数量
2. 卷积核不一定越大越好--3*3的网络
    
    1. 减少了参数量
    2. 提取效果也变好
3. 每层卷积使用多个尺寸的卷积核--**inception**结构
    
    1. 通过将多尺度的特征进行融合，得到更好的feature map
    2. 不足：参数量会多很多
4. 加入**bottleneck**（1*1卷积核）来减少参数
5. ResNet突破了网络越深越难训练的问题
6. 卷积操作不需要同时考虑通道和区域--**DepthWise**
    
    1. 效果好
    2. 减少了参数量
    3. E.g. 输入3，size 3*3，输出256->3*3*3*256
    
    3*3*3--三个通道分别提取DW，过滤器的个数与输入通道数相同，再用256个1*1的 核进行合并成256个输出PW
    
7. 分组卷积中，对通道随机分组--**ShuffleNet**
    
    1. 每次进行通道随机分组，变相实现的特征融合--提高了泛化能力
8. 通道是否平等--**Squeezenet**
    
    1. 我们直接进行加和，但每个通道的重要性不一样
    2. 通过FC和sigmoid，建模出通道之间的相关性--类似于RPN的原理
9. 让固定的卷积核有更大的感受野--空洞卷积（dilated convolution）
10. 卷积核不一定都是矩形--可变形卷积核（deformable）